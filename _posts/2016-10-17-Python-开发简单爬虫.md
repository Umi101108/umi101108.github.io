---
layout: post
title:  "Python 开发简单爬虫"
date:   2016-10-17 
categories: Python
tags: Python 爬虫
excerpt: Python系统回顾爬虫
---
* content
{:toc}


## 课程内容

1. 爬虫简介
2. 简单爬虫架构
3. URL管理器
4. 网页下载器（urllib2）
5. 网页解析器（BeautifulSoup）
6. 完整实例
   * 爬取百度百科Python词条相关的1000个页面数据



## 爬虫是什么

**爬虫**：一段自动抓取互联网信息的程序

**价值**：互联网数据，为我所用！



## 简单爬虫架构

**爬虫调度端** -> [**URL管理器**->**网页下载器**->**网页解析器**->URL管理器]->**价值数据**



### URL 管理器

**URL管理器**：管理待抓取URL集合和已抓取URL集合

* 目的：防止重复抓取、防止循环抓取
* 添加新URL到待爬取集合中
* 判断待添加URL是否在容器中
* 判断是否还有待爬取URL
* 获取待爬取URL
* 将URL从待爬取集合移动到已爬取集合

**实现方式**

* 内存：存储在Python内存中 
  * 待爬取URL集合：set()
  * 已爬取URL集合：set() (因为set集合会去除重复数据)
* 关系数据库：MySQL
  * urls(url, is_crawled)
* 缓存数据库规划



## 网页下载器

**网页下载器**：将互联网上URL对应的网页下载到本地的工具

**Python网页下载器：**

* urllib2 （Python官方基础模块）
* requests （第三方包更强大）



### urllib2

* 下载网页方法1：(最简单)

  ```python
  import urllib2

  # 直接请求
  response = urllib2.urlopen('http://www.baidu.com')

  # 获取状态码，如果是200表示获取成功
  print response.getcode()

  # 读取内容
  cont = response.read()
  ```


* 下载网页方法2：（添加data、http header)

  ```python
  import urllib2

  # 创建Request对象
  request = urllib2.Request(url)

  # 添加数据
  request.add_data('a', '1')
  # 添加http的header
  request.add_header('User-Agent', 'Mozilla/5.0')

  # 发送请求获取结果
  response = urllib2.urlopen(request)
  ```

* 下载网页方法3：（添加特殊情景的处理器)

  需要cookie：使用**HTTPCookieProcessor**

  需要代理：使用**ProxyHandler**

  需要Https加密访问：使用**HTTPSHandler**

  URL自动跳转：使用**HTTPRedirectHandler**

  ```python
  import urllib2, cookielib

  # 创建cookie容器
  cj = cookielib.CookieJar()

  # 创建1个opener
  opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))

  # 给urllib2安装opener
  urllib2.install_opener(opener)

  # 使用带有cookie的urllib2访问网页
  response = urllib2.urlopen("http://www.baidu.com/")

  ```

  ​

## 网页解析器

网页解析器：从网页中提取有价值数据的工具

* 正则表达式  （模糊匹配）
* html.parser  （结构化解析）
* Beautiful Soup  （结构化解析）
* lxml （结构化解析）



结构化解析-DOM（Document Object Model）树



### Beautiful Soup 语法

* 创建BeautifulSoup对象

  ```python
  from bs4 import BeautifulSoup

  # 根据HTML网页字符串创建BeautifulSoup对象
  soup = BeautifulSoup(
  					html_doc,			# HTML文档字符串
  					'html.parser',		# HTML解析器
  					from_encoding='utf8' # HTML文档的编码
  					)
  ```

* 探索节点（find_all, find）

  ```python
  # 方法：find_all(name, attrs, string)

  # 查找所有标签为a的节点
  soup.find_all('a')

  # 查找所有标签为a，链接符合/view/123.htm形式的节点
  soup.find_all('a', href='/view/123.htm')
  soup.find_all('a', href=re.compile(r'/view/\d+\.htm'))

  # 查找所有标签为div，class为abc，文字为python的节点
  soup.find_all('div', class_='abc', string='python')
  ```

* 访问节点信息

  ```python
  # 得到节点：<a href='1.html'>Python</a>

  # 获取查找到的节点的标签名称
  node.name

  # 获取查找到的a节点的href属性
  node['href']

  # 获取查找到的a节点的链接文字
  node.get_text()
  ```



## 实例爬虫

步骤：

* 确定目标
* 分析目标
  * URL格式
  * 数据格式
  * 网页编码
* 编写代码
* 执行爬虫



### 百度百科Python词条相关词条网页-标题和简介

入口页：http://baike.baidu.com/item/Python

URL格式：

— 词条页面URL：/view/125370.htm

数据格式：

— 标题：

* <dd class="lemmaWgt-lemmaTitle-title"><h1>Python</h1></dd>

— 简介：

*  <div class="lemma-summary" label-module="lemmaSummary"> XXX</div>

页面编码：UTF-8